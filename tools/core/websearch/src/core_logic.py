"""
Web Search Tool - Core Logic Module

This module implements the core web search and browsing operations for the FractFlow web search tool.
It provides functions for searching the web using various search engines (Google, Baidu, DuckDuckGo)
and browsing web pages to extract content.

The search engine implementations in the search directory are adapted from https://github.com/FoundationAgents/OpenManus/tree/main/app/tool/search.
The web page browsing functionality is inspired by https://platform.moonshot.cn/docs/guide/use-kimi-api-to-complete-tool-calls#%E6%89%A7%E8%A1%8C%E5%B7%A5%E5%85%B7.


This implementation uses the search engine classes defined in the search directory.

Author: Xinli Xu (xxu068@connect.hkust-gz.edu.cn) - Envision Lab
Date: 2025-05-03
License: MIT License
"""

import asyncio
import requests
import httpx
from typing import List, Dict, Optional, Any, Union
from urllib.parse import urlparse, parse_qs, urljoin
from bs4 import BeautifulSoup
import logging
import sys
import os
import json
import io
from pathlib import Path
from datetime import datetime

# å°è¯•å¯¼å…¥PDFå¤„ç†åº“
try:
    import PyPDF2
    PDF_SUPPORT = True
except ImportError:
    PDF_SUPPORT = False

# Import search engines from search directory
current_dir = Path(__file__).parent
# Add current directory to path so we can import from search directory
sys.path.append(str(current_dir))

# Import search models and engines
from search.base import SearchItem, WebSearchEngine
from search.google_search import GoogleSearchEngine
from search.baidu_search import BaiduSearchEngine
from search.duckduckgo_search import DuckDuckGoSearchEngine

# Constants
MAX_CONTENT_LENGTH = 40000  # Maximum content length in characters


def is_pdf_content(content_type, content):
    """
    æ£€æŸ¥å†…å®¹æ˜¯å¦ä¸ºPDF
    
    Args:
        content_type (str): å†…å®¹ç±»å‹å¤´
        content (bytes): äºŒè¿›åˆ¶å†…å®¹
        
    Returns:
        bool: æ˜¯å¦ä¸ºPDFæ–‡ä»¶
    """
    # æ£€æŸ¥content-type
    if content_type and 'application/pdf' in content_type.lower():
        return True
    
    # æ£€æŸ¥å†…å®¹å¼€å¤´æ˜¯å¦ä¸ºPDFç­¾å
    if content and len(content) > 4 and content[:4] == b'%PDF':
        return True
    
    return False


def extract_text_from_pdf(pdf_content):
    """
    ä»PDFå†…å®¹ä¸­æå–æ–‡æœ¬
    
    Args:
        pdf_content (bytes): PDFæ–‡ä»¶çš„äºŒè¿›åˆ¶å†…å®¹
        
    Returns:
        str: æå–çš„æ–‡æœ¬å†…å®¹
    """
    if not PDF_SUPPORT:
        return "[PDFæ–‡ä»¶: PyPDF2åº“æœªå®‰è£…ï¼Œæ— æ³•è§£æPDFå†…å®¹ã€‚è¯·å®‰è£…PyPDF2: pip install PyPDF2]"
    
    try:
        # åˆ›å»ºä¸€ä¸ªBytesIOå¯¹è±¡
        pdf_file = io.BytesIO(pdf_content)
        
        # åˆ›å»ºPDFé˜…è¯»å™¨
        pdf_reader = PyPDF2.PdfReader(pdf_file)
        
        # æå–æ–‡æœ¬
        text = ""
        for page_num in range(len(pdf_reader.pages)):
            page = pdf_reader.pages[page_num]
            text += page.extract_text() + "\n\n"
        
        if not text.strip():
            return "[PDFæ–‡ä»¶: æ— æ³•æå–æ–‡æœ¬å†…å®¹ï¼Œå¯èƒ½æ˜¯æ‰«æä»¶æˆ–å—ä¿æŠ¤çš„PDF]"
        
        return text
    except Exception as e:
        return f"[PDFæ–‡ä»¶è§£æé”™è¯¯: {str(e)}]"


async def crawl_impl(url: str) -> Dict[str, Any]:
    """
    æ ¹æ®URLè·å–ç½‘é¡µå†…å®¹çš„ç®€å•å®ç°
    
    Args:
        url (str): è¦è·å–å†…å®¹çš„ç½‘é¡µURL
        
    Returns:
        Dict[str, Any]: åŒ…å«å†…å®¹å’Œå…ƒæ•°æ®çš„å­—å…¸
    """
    try:
        # æ£€æŸ¥URLæ ¼å¼
        parsed_url = urlparse(url)
        if not parsed_url.scheme or not parsed_url.netloc:
            return {"error": "æ— æ•ˆçš„URLï¼Œè¯·æä¾›å®Œæ•´URLï¼ŒåŒ…æ‹¬http://æˆ–https://"}
        
        # è®¾ç½®è¯·æ±‚å¤´
        headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/98.0.4758.102 Safari/537.36",
        }
        
        # ä½¿ç”¨httpxè·å–ç½‘é¡µå†…å®¹
        async with httpx.AsyncClient(timeout=10.0, follow_redirects=True) as client:
            response = await client.get(url, headers=headers)
            response.raise_for_status()
            
            # æ£€æŸ¥æ˜¯å¦æ˜¯PDF
            content_type = response.headers.get('content-type', '')
            if is_pdf_content(content_type, response.content):
                # å¦‚æœæ˜¯PDFï¼Œæå–æ–‡æœ¬
                content = extract_text_from_pdf(response.content)
                return {
                    "content": content,
                    "url": url,
                    "is_pdf": True
                }
            
            # å¦‚æœä¸æ˜¯PDFï¼Œè·å–HTMLå†…å®¹
            html_content = response.text
            
            return {
                "content": html_content,
                "url": url,
                "is_pdf": False
            }
            
    except Exception as e:
        return {"error": f"è·å–ç½‘é¡µå†…å®¹æ—¶å‡ºé”™: {str(e)}"}


async def crawl(arguments: dict) -> dict:
    """
    çˆ¬å–ç½‘é¡µå†…å®¹çš„ç®€å•å®ç°
    
    Args:
        arguments (dict): åŒ…å«ä»¥ä¸‹å­—æ®µçš„å­—å…¸:
            - url (str): è¦çˆ¬å–çš„ç½‘é¡µURL
            - max_length (int, optional): è¿”å›å†…å®¹çš„æœ€å¤§é•¿åº¦
    
    Returns:
        dict: åŒ…å«ç½‘é¡µå†…å®¹çš„å­—å…¸
    """
    # è·å–å‚æ•°
    url = arguments.get("url")
    if not url:
        return {"error": "ç¼ºå°‘URLå‚æ•°"}
    
    max_length = arguments.get("max_length", MAX_CONTENT_LENGTH)
    
    # è·å–ç½‘é¡µå†…å®¹
    result = await crawl_impl(url)
    
    # æ£€æŸ¥æ˜¯å¦æœ‰é”™è¯¯
    if "error" in result:
        return {"error": result["error"]}
    
    # é™åˆ¶å†…å®¹å¤§å°
    content = result["content"]
    if len(content) > max_length:
        content = content[:max_length] + f"\n... [å†…å®¹è¢«æˆªæ–­ï¼Œæ€»å…±{len(result['content'])}å­—ç¬¦] ..."
    
    # è¿”å›ç»“æœï¼ŒåŒ…å«æ˜¯å¦ä¸ºPDFçš„ä¿¡æ¯
    is_pdf = result.get("is_pdf", False)
    return {
        "content": content,
        "url": url,
        "is_pdf": is_pdf
    }


# Main search functionality
def format_search_results(search_engine: str, query: str, items: List[SearchItem]) -> str:
    """
    Format search results into a readable string
    
    Args:
        search_engine (str): The name of the search engine used
        query (str): The search query
        items (List[SearchItem]): The search results
        
    Returns:
        str: Formatted search results string
    """
    if not items:
        return f"No results found for '{query}' on {search_engine.capitalize()}"
    
    results = [f"ğŸ” Search results for '{query}' from {search_engine.capitalize()}:"]
    
    for i, item in enumerate(items, 1):
        title = item.title or f"Result {i}"
        url = item.url or "No URL"
        description = item.description or "No description available"
        
        results.append(f"\nğŸ“Œ {title}")
        results.append(f"ğŸ”— {url}")
        results.append(f"ğŸ“„ {description}")
    
    return "\n".join(results)


async def web_search(query: str, search_engine: str = "duckduckgo", num_results: int = 5) -> str:
    """
    Performs a web search and returns relevant results
    
    Args:
        query (str): The keywords or phrase to search for
        search_engine (str, optional): The search engine to use
                                      Options: "duckduckgo", "baidu", "google"
                                      Default is "duckduckgo"
        num_results (int, optional): Number of results to return, default is 5
        
    Returns:
        str: Search results including titles, links and descriptions, or an error message if the search fails
    """
    try:
        # Validate parameters
        if not query:
            return "Search query cannot be empty"
            
        if num_results <= 0:
            return "Number of results must be greater than 0"
        
        if num_results > 20:
            num_results = 20
            
        search_engine = search_engine.lower()
        
        # Initialize search engines
        engines = {
            "google": GoogleSearchEngine(),
            "baidu": BaiduSearchEngine(),
            "duckduckgo": DuckDuckGoSearchEngine()
        }
        
        if search_engine not in engines:
            return f"Unsupported search engine: {search_engine}. Supported options: {', '.join(engines.keys())}"
        
        # Perform search
        engine = engines[search_engine]
        search_results = engine.perform_search(query, num_results=num_results)
        
        # Format and return results
        return format_search_results(search_engine, query, search_results)
            
    except requests.exceptions.RequestException as e:
        return f"Search request failed: {str(e)}"
    except Exception as e:
        return f"An error occurred while performing the search: {str(e)}"


async def web_search_and_browse(query: str, search_engine: str = "duckduckgo", num_results: int = 5, max_browse: int = 1, max_length: int = MAX_CONTENT_LENGTH) -> str:
    """
    æœç´¢å¹¶è‡ªåŠ¨æµè§ˆæœç´¢ç»“æœé¡µé¢
    
    Args:
        query (str): æœç´¢å…³é”®è¯
        search_engine (str): æœç´¢å¼•æ“
        num_results (int): è¿”å›çš„æœç´¢ç»“æœæ•°é‡
        max_browse (int): æœ€å¤šæµè§ˆå‡ ä¸ªæœç´¢ç»“æœ
                         è®¾ç½®ä¸º0è¡¨ç¤ºæµè§ˆæ‰€æœ‰ç»“æœ
                         è®¾ç½®ä¸ºè´Ÿæ•°è¡¨ç¤ºåªæœç´¢ä¸æµè§ˆ
        max_length (int): æ¯ä¸ªç½‘é¡µå†…å®¹çš„æœ€å¤§é•¿åº¦
    
    Returns:
        str: åŒ…å«æœç´¢ç»“æœå’Œç½‘é¡µå†…å®¹çš„ç»¼åˆä¿¡æ¯
    """
    try:
        # é¦–å…ˆæ‰§è¡Œæœç´¢
        search_results = await web_search(query, search_engine, num_results)
        
        # å¦‚æœmax_browseä¸ºè´Ÿæ•°ï¼Œè¡¨ç¤ºåªæ‰§è¡Œæœç´¢ä¸æµè§ˆ
        if max_browse < 0:
            return search_results
        
        # è§£ææœç´¢ç»“æœä¸­çš„URL
        urls = []
        for line in search_results.split('\n'):
            if line.startswith('ğŸ”— '):
                url = line[2:].strip()
                urls.append(url)
        
        if not urls:
            return f"æœç´¢ç»“æœ: {search_results}\n\næ— æ³•ä»æœç´¢ç»“æœä¸­æå–URLã€‚"
        
        # å¦‚æœmax_browseä¸º0ï¼Œåˆ™æµè§ˆæ‰€æœ‰ç»“æœ
        if max_browse == 0:
            max_browse = len(urls)
        
        # é™åˆ¶æµè§ˆçš„URLæ•°é‡
        urls = urls[:max_browse]
        
        # å‡†å¤‡è¾“å‡º
        output = [f"æœç´¢ç»“æœ: {search_results}\n\n--- ç½‘é¡µå†…å®¹ï¼ˆæµè§ˆ {len(urls)}/{len(urls)} ç»“æœï¼‰---\n"]
        
        # çˆ¬å–æ¯ä¸ªURLçš„å†…å®¹
        for i, url in enumerate(urls):
            try:
                # å‡å°å•ä¸ªé¡µé¢çš„æœ€å¤§é•¿åº¦ï¼Œé˜²æ­¢æ€»å†…å®¹è¿‡å¤§
                page_max_length = max(5000, max_length // max(len(urls), 1))
                
                result = await crawl({"url": url, "max_length": page_max_length})
                if "error" in result:
                    page_content = f"âš ï¸ æ— æ³•è·å–å†…å®¹: {result['error']}"
                else:
                    is_pdf = result.get("is_pdf", False)
                    page_content = result["content"]
                    
                output.append(f"\n\n[ç»“æœ {i+1}] - {'[PDFæ–‡ä»¶]' if is_pdf else ''} {url}\n{page_content}")
            except Exception as e:
                output.append(f"\n\n[ç»“æœ {i+1}] - {url}\nâš ï¸ å¤„ç†æ—¶å‡ºé”™: {str(e)}")
        
        return "\n".join(output)
            
    except Exception as e:
        return f"æœç´¢å’Œæµè§ˆè¿‡ç¨‹ä¸­å‡ºé”™: {str(e)}" 